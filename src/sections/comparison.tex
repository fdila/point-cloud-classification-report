From this survey we can see that the main points on which different networks differ is how the input is fed into the network, how each network achieves permutation and transformation invariance, and how each network exploits the spatial information of the point clouds.

\paragraph{Input data} The main difference between the networks is what input they take to be processed.

Projection based methods don't directly use the point clouds, instead they project the point cloud into a different structure, and then feed it to the network.
This makes it easier to design the network, because traditional CNNs can be used to process both images and 3D grids. 
The disadvantages of this approaches are the memory consumption since multiple images and voxels occupy more space than the raw point cloud, and the fact that the projection of the points can introduce artifacts in the images and 3D grids.

Point based methods directly feed the points of the point cloud in the neural network. This solves the disadvantages of projection based methods, but introduces other complications.
The networks must be designed to deal with permutation of the points, transformations of the point cloud and have to specifically design methods to employ the spatial information.

\paragraph{Permutation invariance} Permutation invariance allows the network to be unsensitive to the order of the points in the point cloud.

Projection based methods do not need an explicit way to handle the permutation of the points in the point cloud, since the points get projected either on images or voxels before being fed to the network. The methods used to project the points are invariant to the permutations of the point cloud. 

Point based methods seen in this survey have different ways to achieve this property:
\begin{itemize}
    \item PointNet and PointNet++ use a symmetric function that takes a set of points and produces a new order-invariant vector, such as a max pooling function.
    \item PointConv uses weights that are shared between all the points.
    \item DGCNN uses a symmetric aggregation function over the edge features (the authors used the max function).
\end{itemize}

\paragraph{Transformation invariance} Transformation invariance allows the network to be robust to geometric transformations such as rotation and translation.

Projection based methods share a very similar technique:
\begin{itemize}
    \item MVCNN uses multiple views of the same 3D object to perform the training and the classification of a point cloud. If two inputs of this network represent the same object, but rotated, the architecture will assure an approximate rotational invariance due to the optimization process. It is not intrinsically invariant to translation.
    \item A similar reasoning can be applied to VoxNet, with the only difference that we deal with 3D grids instead of 2D views. As explained in the Voxnet results section and in figure~\ref{fig:VOLUM_invariance}, by analyzing the neuron activations of 12 different rotation of the same object we can show that there is an approximate rotational invariance. It is not intrinsically invariant to translation.
\end{itemize}

Point based methods employ different techniques:
\begin{itemize}
    \item PointNet and PointNet++ use T-Nets to achieve invariance to rigid transformations (both translation and rotation). T-nets predict affine transformation matrices that perform a pose normalization. A T-net predicts the matrix to be multiplied by the input points, and another T-Net predicts the matrix to be multiplied by the features. %TODO è giusto? sì ho aggiunto pose normalization
    \item PointConv is a full approximation of the convolution operator, thus is invariant to translation. It is not intrinsically invariant to rotation.
    \item DGCNN is partially invariant to translation: it is only invariant in the part of the edge function that depends on the difference between the centroid and its neighbors, while the global information in the edge function is not translation invariant. It is not intrinsically invariant to rotation.
\end{itemize}

\paragraph{Spatial information} Since point clouds represent 3D information in space, it can be useful to use the geometric relationship between the points.

Projection based methods project the points into fixed grids and then use standard convolution to compute the features, so they make use of the relationship between the points. 

Point based methods vary depending on the network:

\begin{itemize}
    \item PointNet doesn't use any spatial information to learn the features as it takes the whole point cloud as input.
    \item PointNet++ uses the feature abstraction layer to find local neighbors and then apply PointNet locally to extract a hierarchy of features.
    \item PointConv uses the same feature abstraction layer to find the neighbors and then applies a convolution operator.
    \item DGCNN first computes a local neighborhood graph and then applies a convolution-like operator on the edges of the graph.
\end{itemize}

\paragraph{Results comparison}

The only dataset on which all neural networks have been tested is ModelNet40. In table~\ref{tab:comparison_results} the overall accuracy results and model size are shown. 

It can be seen that the volumetric model has less parameters than to other networks described, but its accuracy is quite low, probably because of the resolution of the voxel grid that must be kept small because of the computational complexity increases cubically. The multi-view approach have fair accuracy results but its model size is 10 to 100 times larger than the other neural networks that achieve similar accuracy.

Point based methods performances have been achieving increasingly better accuracy results, since the first approach described by PointNet was introduced. All the point based methods described have comparable model sizes, apart from PointConv which has $\sim$10 times the parameters of PointNet++ and DGCNN, with similar accuracy results.

\begin{table}[H]
    \centering
    \caption{Results for each network, tested on ModelNet40. The model size is taken from \cite{guo2020deep} for all models except MVCNN, for which the parameters have been extracted from the PyTorch model available on GitHub \url{https://github.com/jongchyisu/mvcnn_pytorch}}
    \begin{tabular}{lcc}
        \hline & \text { MODEL SIZE (params) } & \text { ACCURACY (\%) } \\
        \hline 
        \text { \textbf{MVCNN}} & 128M  & 90.1 \\
        \text { \textbf{VoxNet}} & 921K  & 83.0 \\
        \text { \textbf{PointNet}} & 3.5 M & 89.2 \\
        \text { \textbf{PointNet++}} & 1.48 M & 91.9 \\
        \text { \textbf{PointConv}} & 11 M  & 92.5 \\
        \text { \textbf{DGCNN} } & 1.84 M & 92.9 \\
        \hline
    \end{tabular}
    \label{tab:comparison_results}
\end{table}