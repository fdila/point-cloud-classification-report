Projection-based methods project the unstructured 3D point clouds into specific presupposed modality (e.g. multi-view, voxels, pillars..), and extract features from the target format, which allows them to benefit from the previous research findings in the corresponding direction. In particular we analyze the multi-view representation approach, which can take advantage of the vast literature on computer vision, and the volumetric representation approach.

While intuitively, it seems logical to build 3D shape classifiers directly from 3D models, in this section we'll see how an approach based on 2D images we can actually dramatically outperform classifiers built directly on the 3D representations. In particular, a convolutional neural network trained on a fixed set of rendered views of a 3D shape and only provided with a single view at test time increases category recognition accuracy by a remarkable 8\% (77\% → 85\%) over the best models trained on 3D representations (in 2015). With more views provided at test time, its performance further increases. One of the main reasons is that by using a voxel-based representation the resolution needs to be significantly reduced in order to face the cubic spacial resolution. For example, 3D ShapeNets use a coarse representation of
shape, a 30×30×30 grid of binary voxels. In contrast a single projection of the 3D model of the same input size corresponds to an image of 164×164 pixels, or slightly smaller if multiple projections are used. In addiction to this, we must take into account that using 2D images allow us to benefit from massive image databases, pre-existing CNN architectures which can be fine tuned on our specific domain and a wider literature on image descriptors.