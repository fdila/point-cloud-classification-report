Projection-based methods project the unstructured 3D point clouds into a specific presupposed modality (e.g. multi-view, voxels, pillars..), and extract features from the target format, which allows them to benefit from the previous research findings in the corresponding direction. In particular we analyze the multi-view representation approach, which can take advantage of the vast literature on computer vision, and the volumetric representation approach.

While intuitively it seems logical to build 3D shape classifiers directly from 3D models, in this section it is shown how an approach based on 2D images can actually dramatically outperform classifiers built directly on the 3D representations. In particular, a convolutional neural network trained on a fixed set of rendered views of a 3D shape and only provided with a single view at test time increases category recognition accuracy by a remarkable 8\% (77\% → 85\%) over the best models trained on 3D representations (in 2015). With more views provided at test time, its performance further increases. One of the main reasons for this performance increment is that by using a voxel-based representation the resolution needs to be significantly reduced in order to face the cubic spacial resolution. For example, 3D ShapeNets~\cite{ShapeNets} use a coarse representation of
shape, a 30×30×30 grid of binary voxels. In contrast, a single projection of the 3D model of the same input size corresponds to an image of 164×164 pixels, or slightly smaller if multiple projections are used. In addiction to this, must be taken into account that using 2D images allows to benefit from massive image databases, pre-existing CNN architectures which can be fine tuned and a wide literature on image descriptors.